[
  {
    "objectID": "datasets.html",
    "href": "datasets.html",
    "title": "datasets",
    "section": "",
    "text": "(Many of these routines originally appeared in “audio-diffusion” repo by Zach Evans w/ contributions by Scott Hawley https://github.com/zqevans/audio-diffusion/blob/main/diffusion/utils.py but have since been modified.)",
    "crumbs": [
      "datasets"
    ]
  },
  {
    "objectID": "datasets.html#augmentation-routines",
    "href": "datasets.html#augmentation-routines",
    "title": "datasets",
    "section": "Augmentation routines",
    "text": "Augmentation routines\nThese support ‘pipelining’ in the sense of\n        self.augs = eval(f'torch.nn.Sequential( {augs} )')  \n(see AudioDataset below for sample invocation), to wit: they try to return a similar datatype to what was passed in to .forward(), be it torch.Tensor or dict.\n\nDict pipeline usage\n\n“Get a dict, give a dict” is the incredibly awkward but amazingly accurate summary of this policy.\n\nSome routines may add additional info to a returned dict, if possible.\nReserved keys:\n\n“inputs”: used both for the input and the output of the routine, i.e. “inputs” gets overwritten, i.e. obliterated, i.e. such operations are in-place. If you want to store an unaltered archival copy on an input for later use, then create a new dict key (and maybe even use .clone()).\n\nFor the dict-enabled pipeline, a lot of the return operations will be the same, so…\n\nsource\n\n\npipeline_return\n\n pipeline_return (val, x, key='inputs')\n\nlittle helper routine that appears at end of most augmentations, to compress code\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nval\n\n\nvalue to be returned (by calling function)\n\n\nx\n\n\noriginal data-container that was passed in (tensor or dict)\n\n\nkey\nstr\ninputs\nif x is dict, this key gets overwritten/added\n\n\n\n\nsource\n\n\nRandomGain\n\n RandomGain (min_gain, max_gain)\n\napply a random gain to audio\n\n\n\n\nDetails\n\n\n\n\nmin_gain\nminimum gain to apply\n\n\nmax_gain\nmaximum gain to apply\n\n\n\nTesting RandomGain:\n\naudio = torch.rand(8)\nprint(f\"audio = {audio}\")\ngain_op = RandomGain(-2.0,2.0)\naudio2 = gain_op(audio)  # audio does not get overwritten\nprint(f\"audio NOT OVERWRITTEN = {audio}\\naudio2 = {audio2}\\n\")\n\naudio = tensor([0.3984, 0.7110, 0.4340, 0.0856, 0.5396, 0.9248, 0.8500, 0.4580])\naudio NOT OVERWRITTEN = tensor([0.3984, 0.7110, 0.4340, 0.0856, 0.5396, 0.9248, 0.8500, 0.4580])\naudio2 = tensor([-0.2905, -0.5185, -0.3165, -0.0624, -0.3935, -0.6744, -0.6198, -0.3340])\n\n\n\nNote how, with the dict version of the pipeline, the inputs element of the dict gets overwritten:\n\nx = {'inputs':audio}\nprint(f\"x['inputs'] = {x['inputs']}\")\naudio2 = gain_op(x)  # x['inputs'] gets overwritten but audio does not\nprint(f\"audio NOT OVERWRITTEN = {audio}\\nx['inputs'] OVERWRITTEN = {x['inputs']} \")\nprint(f\"audio2 = {audio2}\")\nassert torch.equal(audio2['inputs'], x['inputs']), \"Oh no.  NO idea what's going on\"\nprint(\"\\naudio2['inputs'] == x['inputs']: True, i.e. x['inputs'] was overwritten\")\n\nx['inputs'] = tensor([0.3984, 0.7110, 0.4340, 0.0856, 0.5396, 0.9248, 0.8500, 0.4580])\naudio NOT OVERWRITTEN = tensor([0.3984, 0.7110, 0.4340, 0.0856, 0.5396, 0.9248, 0.8500, 0.4580])\nx['inputs'] OVERWRITTEN = tensor([-0.7592, -1.3550, -0.8272, -0.1631, -1.0283, -1.7625, -1.6198, -0.8728]) \naudio2 = {'inputs': tensor([-0.7592, -1.3550, -0.8272, -0.1631, -1.0283, -1.7625, -1.6198, -0.8728])}\n\naudio2['inputs'] == x['inputs']: True, i.e. x['inputs'] was overwritten\n\n\n\nsource\n\n\nPadCrop\n\n PadCrop (n_samples, randomize=True, redraw_silence=True,\n          silence_thresh=-60, max_redraws=2)\n\nGrabs a randomly-located section from an audio file, padding with zeros in case of any misalignment\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_samples\n\n\nlength of chunk to extract from longer signal\n\n\nrandomize\nbool\nTrue\ndraw cropped chunk from a random position in audio file\n\n\nredraw_silence\nbool\nTrue\na chunk containing silence will be replaced with a new one\n\n\nsilence_thresh\nint\n-60\nthreshold in dB below which we declare to be silence\n\n\nmax_redraws\nint\n2\nwhen redrawing silences, don’t do it more than this many\n\n\n\nVariation on PadCrop. source: Zach Evan’s audio-diffusion repo:\n\nsource\n\n\nPadCrop_Normalized_T\n\n PadCrop_Normalized_T (n_samples:int, sample_rate:int,\n                       randomize:bool=True)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\nPadCrop_Normalized_T_old\n\n PadCrop_Normalized_T_old (n_samples:int, randomize:bool=True)\n\nVariation on PadCrop. source: Zach Evan’s audio-diffusion repo\nTesting PadCrop()\n\naudio = torch.rand(8)\ncrop_op = PadCrop(3)\ntorch.random.manual_seed(0)\ncrop1 = crop_op(audio)\nprint(\"audio = \",audio)\nprint(\"crop1 = \",crop1)                        # raw tensor version\n\ntorch.random.manual_seed(0)\ncrop_dict = crop_op({'inputs':audio})   # dict version\nprint(\"crop_dict = \",crop_dict)   # dict version\n\nassert torch.equal(crop1, crop_dict['inputs']), f\"These should be equal: {crop1}, {crop_dict['inputs']}\"\nprint('crop1 == crop_dict: Success!')\n\naudio =  tensor([0.7682, 0.0885, 0.1320, 0.3074, 0.6341, 0.4901, 0.8964, 0.4556])\ncrop1 =  tensor([[0.1320, 0.3074, 0.6341]])\ncrop_dict =  {'inputs': tensor([[0.1320, 0.3074, 0.6341]]), 'crop_range': tensor([2, 5])}\ncrop1 == crop_dict: Success!\n\n\nAnd test the dict version:\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\nx = {'inputs':torch.rand(8).to(device)}\ncrop_op = PadCrop(3)\ntorch.random.manual_seed(0)\ncrop = crop_op(x)\nprint(\"crop =\",crop)\n\ncrop = {'inputs': tensor([[0.1320, 0.3074, 0.6341]], device='cuda:0'), 'crop_range': tensor([2, 5], device='cuda:0')}\n\n\n\nsource\n\n\nPhaseFlipper\n\n PhaseFlipper (p=0.5)\n\nshe was PHAAAAAAA-AAAASE FLIPPER, a random invert yeah\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\np\nfloat\n0.5\nprobability that phase flip will be applied\n\n\n\n\nsource\n\n\nFillTheNoise\n\n FillTheNoise (p=0.33)\n\nrandomly adds a bit of noise, or not, just to spice things up. (Name is an homage to DJ/artist/collaborator Kill the Noise)\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\np\nfloat\n0.33\nprobability that noise will be added\n\n\n\n\nsource\n\n\nRandPool\n\n RandPool (p=0.2)\n\nmaybe (or maybe not) do an avgpool operation, with a random-sized kernel\n\nsource\n\n\nNormInputs\n\n NormInputs (do_norm=True)\n\nNormalize inputs to [-1,1]. Useful for quiet inputs\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndo_norm\nbool\nTrue\ncontrollable parameter for turning normalization on/off\n\n\n\n\nsource\n\n\nMono\n\n Mono (*args, **kwargs)\n\nconvert audio to mono\n\nsource\n\n\nStereo\n\n Stereo (*args, **kwargs)\n\nconvert audio to stereo",
    "crumbs": [
      "datasets"
    ]
  },
  {
    "objectID": "datasets.html#masking-of-inputs",
    "href": "datasets.html#masking-of-inputs",
    "title": "datasets",
    "section": "Masking (of inputs)",
    "text": "Masking (of inputs)\nFirst a couple utility routines before the main masking routine:\n\nsource\n\nsmoothstep\n\n smoothstep (x, edge0=0.4, edge1=0.6)\n\nan s-shaped curve, 0’s on left side and 1’s at right side, with gradient zero at all 1’s and 0’s. cf. https://en.wikipedia.org/wiki/Smoothstep\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nx\n\n\na tensor of coordinates across a domain, e.g. [0,1]\n\n\nedge0\nfloat\n0.4\n“zero”/“left” side of smoothstep\n\n\nedge1\nfloat\n0.6\n“one”/“right” side of smoothstep\n\n\n\n\nsource\n\n\nsmoothstep_box\n\n smoothstep_box (coords, edges=(0.2, 0.3, 0.5, 0.6))\n\nmakes a flat region of zeros that transitions smoothly to 1’s via smoothsteps at the sides\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncoords\n\n\ntensor of coordinate values\n\n\nedges\ntuple\n(0.2, 0.3, 0.5, 0.6)\n(left 1’s boundary, left 0’s boundary, right 0’s boundary, right 1’s boundary)\n\n\n\nTesting smoothstep_box:\n\nimport matplotlib.pyplot as plt\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\nprint(f\"device = {device}\")\nx = torch.linspace(0,1,steps=100).to(device)\ny = smoothstep_box(x)\nplt.plot(x.cpu(), y.cpu())\nplt.xlabel('x')\nplt.show()\n\ndevice = cuda\n\n\n\n\n\n\n\n\n\nAnd now the main masking routine:\n\nsource\n\n\nRandMask1D\n\n RandMask1D (mask_frac=0.25, mask_width=0.1, mask_type='simple',\n             edge_width=0.2, per_channel=False, verbose=False)\n\nPerforms masking or ‘cutout’ along 1d data. Can support ‘smooth sides’ to the cutouts. Note that you probably want masking to be the last* step in the augmentation pipeline*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmask_frac\nfloat\n0.25\nfraction of total input that is to be masked (helps compute no. of masked regions)\n\n\nmask_width\nfloat\n0.1\neither a fraction of the total length (float &lt; 1) or an exact integer value for length of each masked region\n\n\nmask_type\nstr\nsimple\n‘simple’=hard sides to cuts, ‘softstep’=smooth sides, ‘nyquist’=nyquist-freq wave 0.5*(1,-1,1,-1,..)\n\n\nedge_width\nfloat\n0.2\nfor mask_type=smoothstep, fraction or integer value of transition regions to come in from the sides of zeros region\n\n\nper_channel\nbool\nFalse\ndifferent masks on different channels; model can cheat if your inputs are mono\n\n\nverbose\nbool\nFalse\nshow logging info\n\n\n\nLet’s test the simple mask (“hard cuts”):\n\ntorch.manual_seed(0)\naudio = (2*torch.rand((2,2,5000))-1)#.to(device)\nmask_op = RandMask1D(mask_frac=0.3, mask_width=0.1, verbose=True)\nmasked = mask_op.forward(audio)\n\n# routine to display what we got\ndef display_mask_data(audio, mask_op, masked):\n    fig, ax = plt.subplots(1,3,figsize=(14,4))\n    ax[1].plot(mask_op.mask.cpu(), label='single mask')\n    for c in range(audio.shape[1]): # show different channels of masked audio\n        ax[0].plot(audio[0,0,:].cpu(),  alpha=0.4, label=f'audio, channel{c}')\n        ax[2].plot(masked[0,c,:].cpu(), alpha=0.4, label=f'masked audio, channel{c}')\n    for i in range(3): ax[i].legend()\n    plt.show() \n\ndisplay_mask_data(audio, mask_op, masked)\n\n\n MMMM-  RandMask1D: Mask engaged!  self.mask_width, self.n_masks =  500 3 \n\n\n\n\n\n\n\n\n\n\nNow test the mask with “softstep” sides and the “per-channel” masking:\n\nmask_op = RandMask1D(mask_frac=0.4, mask_width=0.2, mask_type='smoothstep', edge_width=0.3, verbose=True, per_channel=True)\nmasked = mask_op.forward(audio)\ndisplay_mask_data(audio, mask_op, masked)\n\n\n MMMM-  RandMask1D: Mask engaged!  self.mask_width, self.n_masks =  1000 2 \n\n\n\n\n\n\n\n\n\n\n…and lets make sure the dict version retains the “unmasked” audio, unaltered:\n\nmasked = mask_op.forward({'inputs':audio})\ndisplay_mask_data(masked['unmasked'], mask_op, masked['inputs'] )\n\n\n\n\n\n\n\n\nThe idea behind the Nyquist freq replacement is that it could perhaps serve as a “mask code” that is different from (musically relevant) silence. And hopefully the neural network picks up on it while the human ear does not!\n\nmask_op = RandMask1D(mask_frac=0.3, mask_width=0.3, mask_type='nyquist', verbose=True)\nmasked = mask_op.forward(audio)\ndisplay_mask_data(audio, mask_op, masked)\n\n\n MMMM-  RandMask1D: Mask engaged!  self.mask_width, self.n_masks =  1500 1",
    "crumbs": [
      "datasets"
    ]
  },
  {
    "objectID": "datasets.html#webdataset-support",
    "href": "datasets.html#webdataset-support",
    "title": "datasets",
    "section": "WebDataset support",
    "text": "WebDataset support\n\nBackground Info\nRefer to the official WebDataset Repo on GitHub.\n\nWebDataset makes it easy to write I/O pipelines for large datasets. Datasets can be stored locally or in the cloud.\n\nThey use the word “shards” but never define what “shard” means. I (S.H.) surmise they mean the groups of data files which are gathered into a series of .tar files – the .tar files are the shards?\ncf. Video Tutorial: “Loading Training Data with WebDataset”.\nThe recommended usage for AWS S3 can be seen in [this GitHub Issue comment by tmbdev] (https://github.com/webdataset/webdataset/issues/21#issuecomment-706008342):\nurl = \"pipe:s3cmd get s3://bucket/dataset-{000000..000999}.tar -\"\ndataset = wds.Dataset(url)...\n\n1 s3cmd get should read aws s3 cp.\n\nThat URL is expecting a contiguously-numbered range of .tar files. So if the file numbers are contiguous (no gaps), then we’ll have an easy time. Otherwise, there are ways to pass in a long list of similar “pipe:…tar” ‘urls’ for each and every tar file, which is still not a big deal though it may appear messier.\nAWS hates double slashes, so we’ll use the following\n\nsource\n\n\nfix_double_slashes\n\n fix_double_slashes (s, debug=False)\n\naws is pretty unforgiving compared to ‘normal’ filesystems. so here’s some ‘cleanup’\nTest that:\n\ns = 's3://hey///ho//lets/go'\nprint(fix_double_slashes(s, debug=True))\n\ns3://hey/ho/lets/go\n\n\n\nNOTE: be prepared for extensive ‘testing cases’ shown for the following routines.\n\n\n\nGeneral utility: get_s3_contents()\n\nsource\n\n\nget_s3_contents\n\n get_s3_contents (dataset_path, s3_url_prefix='s3://s-laion-\n                  audio/webdataset_tar/', filter='', recursive=True,\n                  debug=False, profile='default')\n\nGets a list of names of files or subdirectories on an s3 path\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndataset_path\n\n\n“name” of the dataset on s3\n\n\ns3_url_prefix\nstr\ns3://s-laion-audio/webdataset_tar/\ns3 bucket to check\n\n\nfilter\nstr\n\nonly grab certain filename / extensions\n\n\nrecursive\nbool\nTrue\ncheck all subdirectories. RECOMMEND LEAVING THIS TRUE\n\n\ndebug\nbool\nFalse\nprint debugging info (don’t rely on this info staying consistent)\n\n\nprofile\nstr\ndefault\nname of the AWS profile credentials\n\n\n\nLet’s test that on the FSD50K dataset: &gt; Note: These tests will only work on systems on which you have valid AWS credentials for the S3 buckets in question. If the docs show a bunch of blanks in what follows, it’s because they were generated on a system without such credentials.\n\nget_s3_contents('130000_MIDI_SONGS')[:10]\n\n['webdataset_tar/130000_MIDI_SONGS/130000_Pop_Rock_Classical_Videogame_EDM_MIDI_Archive[6_19_15]/test/sizes.json',\n 'webdataset_tar/130000_MIDI_SONGS/130000_Pop_Rock_Classical_Videogame_EDM_MIDI_Archive[6_19_15]/train/sizes.json',\n 'webdataset_tar/130000_MIDI_SONGS/2/test/0.tar',\n 'webdataset_tar/130000_MIDI_SONGS/2/test/sizes.json',\n 'webdataset_tar/130000_MIDI_SONGS/2/train/0.tar',\n 'webdataset_tar/130000_MIDI_SONGS/2/train/1.tar',\n 'webdataset_tar/130000_MIDI_SONGS/2/train/2.tar',\n 'webdataset_tar/130000_MIDI_SONGS/2/train/3.tar',\n 'webdataset_tar/130000_MIDI_SONGS/2/train/4.tar',\n 'webdataset_tar/130000_MIDI_SONGS/2/train/5.tar']\n\n\n\nget_s3_contents('FSD50K/test/')[:10]\n\n['webdataset_tar/FSD50K/test/0.tar',\n 'webdataset_tar/FSD50K/test/1.tar',\n 'webdataset_tar/FSD50K/test/10.tar',\n 'webdataset_tar/FSD50K/test/11.tar',\n 'webdataset_tar/FSD50K/test/12.tar',\n 'webdataset_tar/FSD50K/test/13.tar',\n 'webdataset_tar/FSD50K/test/14.tar',\n 'webdataset_tar/FSD50K/test/15.tar',\n 'webdataset_tar/FSD50K/test/16.tar',\n 'webdataset_tar/FSD50K/test/17.tar']\n\n\nAnd let’s try filtering for only tar files:\n\ntar_names = get_s3_contents('FSD50K/test', filter='.tar')\ntar_names\n\n['webdataset_tar/FSD50K/test/0.tar',\n 'webdataset_tar/FSD50K/test/1.tar',\n 'webdataset_tar/FSD50K/test/10.tar',\n 'webdataset_tar/FSD50K/test/11.tar',\n 'webdataset_tar/FSD50K/test/12.tar',\n 'webdataset_tar/FSD50K/test/13.tar',\n 'webdataset_tar/FSD50K/test/14.tar',\n 'webdataset_tar/FSD50K/test/15.tar',\n 'webdataset_tar/FSD50K/test/16.tar',\n 'webdataset_tar/FSD50K/test/17.tar',\n 'webdataset_tar/FSD50K/test/18.tar',\n 'webdataset_tar/FSD50K/test/19.tar',\n 'webdataset_tar/FSD50K/test/2.tar',\n 'webdataset_tar/FSD50K/test/3.tar',\n 'webdataset_tar/FSD50K/test/4.tar',\n 'webdataset_tar/FSD50K/test/5.tar',\n 'webdataset_tar/FSD50K/test/6.tar',\n 'webdataset_tar/FSD50K/test/7.tar',\n 'webdataset_tar/FSD50K/test/8.tar',\n 'webdataset_tar/FSD50K/test/9.tar']\n\n\nList all LAION audio datasets:\n\nget_s3_contents('',recursive=False)[:20]\n\n['130000_MIDI_SONGS/',\n 'Audiostock_music/',\n 'BBCSoundEffects/',\n 'CMU_Arctic/',\n 'CREMA-D/',\n 'Cambridge_mt/',\n 'Clotho/',\n 'CoVoST_2/',\n 'ESC50_1/',\n 'ESC50_2/',\n 'ESC50_3/',\n 'ESC50_4/',\n 'ESC50_5/',\n 'EmoV_DB/',\n 'Europarl-st/',\n 'FMA/',\n 'FMA_stereo/',\n 'FMA_updated/',\n 'FSD50K/',\n 'GTZAN/']\n\n\n\n\nFor contiguous file-number lists…\nMaybe the range of tar numbers is contigous. (In the LAION AudoiDataset archives, they are each contiguous within train/, valid/, and test/ subsets.) If so, let’s have something to output that range:\n\nsource\n\n\nget_contiguous_range\n\n get_contiguous_range (tar_names)\n\ngiven a string of tar file names, return a string of their numerical range if the numbers are contiguous. Otherwise return empty string\n\n\n\n\n\n\n\n\nDetails\n\n\n\n\ntar_names\nlist of tar file names, although the .tar part is actually optional\n\n\n\n\ncont_range = get_contiguous_range(tar_names)\ncont_range\n\n'{0..19}'\n\n\nTest if leading zeros are preserved:\n\nget_contiguous_range(['0000'+x for x in tar_names])\n\n'{0..19}'\n\n\nTest zero-element and single element versions:\n\nprint(get_contiguous_range([]))\nprint(get_contiguous_range([1]))\n\n\n1\n\n\nAnd show that ‘.tar’ is optional:\n\nget_contiguous_range(['01','02','3'])\n\n'{01..3}'\n\n\n….So, if a contiguous range of tar file names is available in a WebDataset directory, then we can just use the native WebDataset creation utilities and can ignore all the other %$#*& that’s about to follow below.\nLet’s test the simple version first:\n\ns3_url_prefix='s3://s-laion-audio/webdataset_tar/'\nurl = f\"pipe:aws s3 cp {s3_url_prefix}FSD50K/test/{cont_range}.tar -\"  # 'aws get' is not a thing. 'aws cp' is\nprint(url)\ndataset = wds.WebDataset(url)\n\npipe:aws s3 cp s3://s-laion-audio/webdataset_tar/FSD50K/test/{0..19}.tar -\n\n\nWebDataset is a kind of IterableDataset, so we can iterate over it directly:\n\n## NOTE TO SELF: DON'T RUN THIS ON STABILITY CLUSTER HEADNODE (But Jupyter nodes are fine)\ntry:\n    for sample in dataset:  \n        for k,v in sample.items():  # print the all entries in dict\n            print(f\"{k:20s} {repr(v)[:50]}\")\n        break                       # abort after first dict\nexcept:\n    sample = None\n\n__key__              './mnt/audio_clip/processed_datasets/FSD50K/test/3\n__url__              'pipe:aws s3 cp s3://s-laion-audio/webdataset_tar/\nflac                 b'fLaC\\x00\\x00\\x00\"\\x12\\x00\\x12\\x00\\x00\\x0ee\\x00\\x\njson                 b'{\"text\": [\"The sounds of Aircraft, Engine, Fixed\n\n\n\nif sample: \n    audio_keys = (\"flac\")\n    found_key, rewrite_key = '', ''\n    for k,v in sample.items():  \n        for akey in audio_keys:\n            if k.endswith(akey): \n                found_key, rewrite_key = k, akey\n                break\n        if '' != found_key: break \n    if '' == found_key:  # got no audio!   \n        print(\"Error: No audio in this sample:\")\n        for k,v in sample.items():  # print the all entries in dict\n            print(f\"{k:20s} {repr(v)[:50]}\")\n    else:\n        print(\"Found flac\")\n        flac = sample['flac']\n\nFound flac\n\n\nThere’s a built-in decoder for various audio formats, so we can just use:\n\nfrom aeiou.viz import audio_spectrogram_image\nfrom IPython.display import display \nif sample:\n    dataset = wds.WebDataset(url).decode(wds.torch_audio) # throw out the json\n    sample = next(iter(dataset))\n    audio, sr = (sample[\"flac\"])\n    audio = audio[:,:min(audio.shape[-1], 128000)]\n    print('audio.shape = ',audio.shape)\n    #(audio, specs='wave_mel', output_type='live')\n    spec_graph = audio_spectrogram_image(audio, justimage=False, db=False, db_range=[-60,20])\n    display(spec_graph)\n\naudio.shape =  torch.Size([1, 128000])\n\n\n/fsx/shawley/envs_sm/aa/lib/python3.10/site-packages/torchaudio/transforms/_transforms.py:611: UserWarning: Argument 'onesided' has been deprecated and has no influence on the behavior of this module.\n  warnings.warn(\n/fsx/shawley/envs_sm/aa/lib/python3.10/site-packages/torchaudio/functional/functional.py:576: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (513) may be set too low.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\nNon-contiguously-numbered lists of tar files…\nHere we’ll just get individual URLs for every tar file possible for a given list of dataset names\n\nsource\n\n\nget_all_s3_urls_zach\n\n get_all_s3_urls_zach (names=[], subsets=[''], s3_url_prefix=None,\n                       recursive=True, filter_str='tar', debug=False,\n                       profiles={})\n\nget urls of shards (tar files) for multiple datasets in one s3 bucket\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnames\nlist\n[]\nlist of all valid [LAION AudioDataset] dataset names\n\n\nsubsets\nlist\n[’’]\nlist of subsets you want from those datasets, e.g. [‘train’,‘valid’]\n\n\ns3_url_prefix\nNoneType\nNone\nprefix for those dataset names\n\n\nrecursive\nbool\nTrue\nrecursively list all tar files in all subdirs\n\n\nfilter_str\nstr\ntar\nonly grab files with this substring\n\n\ndebug\nbool\nFalse\nprint debugging info – note: info displayed likely to change at dev’s whims\n\n\nprofiles\ndict\n{}\ndictionary of profiles for each item in names, e.g. {‘dataset1’: ‘profile1’, ‘dataset2’: ‘profile2’}\n\n\n\n\nsource\n\n\nget_all_s3_urls\n\n get_all_s3_urls (names=[], subsets=[''], s3_url_prefix=None,\n                  recursive=True, filter_str='tar', debug=False,\n                  profiles={}, **kwargs)\n\nget urls of shards (tar files) for multiple datasets in one s3 bucket\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnames\nlist\n[]\nlist of all valid [LAION AudioDataset] dataset names, can include URLs in which case s3_url_prefix is ignored\n\n\nsubsets\nlist\n[’’]\nlist of subsets you want from those datasets, e.g. [‘train’,‘valid’]\n\n\ns3_url_prefix\nNoneType\nNone\nprefix for those dataset names if no s3:// supplied in names, e.g. ‘s3://s-laion-audio/webdataset_tar/’\n\n\nrecursive\nbool\nTrue\nrecursively list all tar files in all subdirs\n\n\nfilter_str\nstr\ntar\nonly grab files with this substring\n\n\ndebug\nbool\nFalse\nprint debugging info – note: info displayed likely to change at dev’s whims\n\n\nprofiles\ndict\n{}\nlist of S3 profiles to use, e.g. {‘s3://s-laion-audio’:‘default’}\n\n\nkwargs\n\n\n\n\n\n\n\nnames = [\n        #\"s3://s-harmonai/datasets/songs_raw/songs_md_16bit_mono/\",\n        # \"s3://s-harmonai/datasets/samples_raw/samples_all/1/\",\n        # \"s3://s-harmonai/datasets/samples_raw/samples_ms/1/\",\n        # \"s3://s-laion-audio/webdataset_tar/freesound_no_overlap\", \n        \"s3://s-laion-audio/webdataset_tar/FMA_stereo/\"\n    ]\n\n    print(\"Getting URL list...\")\n    urls = get_all_s3_urls(\n        profiles={'s3://s-harmonai':'scott'},\n        names=names, \n        s3_url_prefix=None,\n        recursive=True, debug=False,\n    )\n    print(\"len(urls) =\",len(urls))\n\nGetting URL list...\nlen(urls) = 838\n\n\n\nurls[:5]\n\n['pipe:aws s3 --cli-connect-timeout 0 cp s3://s-laion-audio/webdataset_tar/FMA_stereo/test - --profile default',\n 'pipe:aws s3 --cli-connect-timeout 0 cp s3://s-laion-audio/webdataset_tar/FMA_stereo/test/0.tar - --profile default',\n 'pipe:aws s3 --cli-connect-timeout 0 cp s3://s-laion-audio/webdataset_tar/FMA_stereo/test/1.tar - --profile default',\n 'pipe:aws s3 --cli-connect-timeout 0 cp s3://s-laion-audio/webdataset_tar/FMA_stereo/test/10.tar - --profile default',\n 'pipe:aws s3 --cli-connect-timeout 0 cp s3://s-laion-audio/webdataset_tar/FMA_stereo/test/11.tar - --profile default']\n\n\n\nsource\n\n\nIterableAudioDataset\n\n IterableAudioDataset (paths, sample_rate=48000, sample_size=65536,\n                       random_crop=True, load_frac=1.0,\n                       cache_training_data=False, num_gpus=8,\n                       redraw_silence=True, silence_thresh=-60,\n                       max_redraws=2, augs='Stereo(), PhaseFlipper()',\n                       verbose=False)\n\nIterable version of AudioDataset, used with Chain (below)\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npaths\n\n\nlist of strings of directory (/tree) names to draw audio files from\n\n\nsample_rate\nint\n48000\naudio sample rate in Hz\n\n\nsample_size\nint\n65536\nhow many audio samples in each “chunk”\n\n\nrandom_crop\nbool\nTrue\ntake chunks from random positions within files\n\n\nload_frac\nfloat\n1.0\nfraction of total dataset to load\n\n\ncache_training_data\nbool\nFalse\nTrue = pre-load whole dataset into memory (not fully supported)\n\n\nnum_gpus\nint\n8\nused only when cache_training_data=True, to avoid duplicates,\n\n\nredraw_silence\nbool\nTrue\na chunk containing silence will be replaced with a new one\n\n\nsilence_thresh\nint\n-60\nthreshold in dB below which we declare to be silence\n\n\nmax_redraws\nint\n2\nwhen redrawing silences, don’t do it more than this many\n\n\naugs\nstr\nStereo(), PhaseFlipper()\nlist of augmentation transforms after PadCrop, as a string\n\n\nverbose\nbool\nFalse\nwhether to print notices of reasampling or not\n\n\n\n\nfrom aeiou.viz import playable_spectrogram\n\n\niter_ds = IterableAudioDataset('/fsx/shawley/data/maestro', augs='Stereo(), NormInputs()')\nassert isinstance(iter_ds, torch.utils.data.IterableDataset),\"Nope\"\ntry:\n    sample = next(iter(iter_ds))\n    playable_spectrogram(sample, specs='wave_mel', output_type='live')\nexcept: pass\n\naugs = Stereo(), NormInputs()\nAudioDataset:1276 files found.\n\n\n\n\n\n\n\n\n\n\n\n/fsx/shawley/envs_sm/aa/lib/python3.10/site-packages/torchaudio/transforms/_transforms.py:611: UserWarning: Argument 'onesided' has been deprecated and has no influence on the behavior of this module.\n  warnings.warn(",
    "crumbs": [
      "datasets"
    ]
  },
  {
    "objectID": "datasets.html#audiowebdataloader",
    "href": "datasets.html#audiowebdataloader",
    "title": "datasets",
    "section": "AudioWebDataLoader",
    "text": "AudioWebDataLoader\nUses WebDataset for audio files\n\nsource\n\nwds_preprocess\n\n wds_preprocess (sample, sample_size=65536, sample_rate=48000,\n                 verbose=False, random_crop=True, normalize_lufs=None,\n                 metadata_prompt_funcs=None, force_channels='stereo',\n                 augment_phase=True)\n\nutility routine for QuickWebDataLoader, below. New version by Zach Evans, from https://github.com/zqevans/audio-diffusion/dataset.py. Old version in source, commented out\n\nsource\n\n\nname_cache_file\n\n name_cache_file (url)\n\nprovides the filename to which to cache a url",
    "crumbs": [
      "datasets"
    ]
  },
  {
    "objectID": "datasets.html#simple-version-get_wds_loader",
    "href": "datasets.html#simple-version-get_wds_loader",
    "title": "datasets",
    "section": "Simple version: get_wds_loader",
    "text": "Simple version: get_wds_loader\nA simple routine for basic pulling of audio files\n\nsource\n\nget_wds_loader\n\n get_wds_loader (batch_size, sample_size, names, s3_url_prefix=None,\n                 sample_rate=48000, num_workers=8, recursive=True,\n                 profiles={}, epoch_steps=1000, random_crop=True,\n                 normalize_lufs=None, metadata_prompt_funcs=None,\n                 force_channels='stereo', augment_phase=True)\n\nSimpler loader from https://github.com/zqevans/audio-diffusion/dataset.py\nTest code for get_wds_loader:\n\nbatch_size = 4\nsample_size = 2**18\nsample_rate = 48000\nnum_workers = 4\nprofiles = {}\n\ndl = get_wds_loader(\n        batch_size=batch_size,\n        s3_url_prefix=None,\n        sample_size=sample_size,\n        names=names,\n        sample_rate=sample_rate,\n        num_workers=num_workers,\n        recursive=True,\n        random_crop=True,\n        epoch_steps=1,\n        profiles=profiles,\n)\ndl_iter = iter(dl)\nbatch = next(dl_iter) \nprint(\"batch = \",batch) \nprint(\"Success!\")\n\ndownload failed: s3://s-laion-audio/webdataset_tar/FMA_stereo/train/642.tar to - [Errno 32] Broken pipe\ndownload failed: s3://s-laion-audio/webdataset_tar/FMA_stereo/train/82.tar to - [Errno 32] Broken pipe\ndownload failed: s3://s-laion-audio/webdataset_tar/FMA_stereo/test/35.tar to - [Errno 32] Broken pipe\ndownload failed: s3://s-laion-audio/webdataset_tar/FMA_stereo/train/269.tar to - [Errno 32] Broken pipe\n\n\nbatch =  [tensor([[[[ 0.4060,  0.3965,  0.3820,  ...,  0.5274,  0.5349,  0.5406],\n          [ 0.5739,  0.5558,  0.5435,  ...,  0.5655,  0.5707,  0.5763]],\n\n         [[-0.3591, -0.3761, -0.3705,  ..., -0.2486, -0.3406, -0.4426],\n          [-0.2027, -0.1295, -0.0775,  ..., -0.3002, -0.3320, -0.3603]],\n\n         [[-0.1131, -0.0873, -0.1157,  ...,  0.0386,  0.0382,  0.0378],\n          [-0.2008, -0.1766, -0.1978,  ...,  0.0127,  0.0131,  0.0133]],\n\n         [[ 0.1581,  0.0486, -0.0798,  ..., -0.0239, -0.1204, -0.1762],\n          [ 0.0942,  0.0589,  0.0190,  ..., -0.1309, -0.1081, -0.0622]]]]), [{'text': ['playing song in album The Cult From Moon Mountain, titled Circle Moon, by Fursaxa, of which the genre is Folk, the language code is en, the composer is Tara Burke, the date created is 2008-11-26 02:13:59'], 'original_data': {'title': ['FMA: A Dataset For Music Analysis'], 'description': ['Free Music Archive (FMA), an open and easily accessible dataset suitable for evaluating several tasks in MIR, a field concerned with browsing, searching, and organizing large music collections.'], 'license': ['MIT License'], 'filename': ['000714.mp3'], 'genre': ['Folk'], 'album': ['The Cult From Moon Mountain'], 'song_title': ['Circle Moon'], 'artist': ['Fursaxa'], 'duration': tensor([338]), 'composer': ['Tara Burke'], 'date_recorded': ['2008-11-26 02:13:59'], 'language_code': ['en']}, 'seconds_start': tensor([69]), 'seconds_total': tensor([339]), 'prompt': ['playing song in album The Cult From Moon Mountain, titled Circle Moon, by Fursaxa, of which the genre is Folk, the language code is en, the composer is Tara Burke, the date created is 2008-11-26 02:13:59']}, {'text': [\"playing song titled I'm Quitting, in album Hank IV Live at WFMU on Brian's Show on 11/18/2008, by Hank IV, of which the language code is en, the date created is 2008-12-04 20:08:12, the genre is Rock\"], 'original_data': {'title': ['FMA: A Dataset For Music Analysis'], 'description': ['Free Music Archive (FMA), an open and easily accessible dataset suitable for evaluating several tasks in MIR, a field concerned with browsing, searching, and organizing large music collections.'], 'license': ['MIT License'], 'filename': ['003688.mp3'], 'genre': ['Rock'], 'album': [\"Hank IV Live at WFMU on Brian's Show on 11/18/2008\"], 'song_title': [\"I'm Quitting\"], 'artist': ['Hank IV'], 'duration': tensor([146]), 'composer': ['nan'], 'date_recorded': ['2008-12-04 20:08:12'], 'language_code': ['en']}, 'seconds_start': tensor([38]), 'seconds_total': tensor([147]), 'prompt': [\"playing song titled I'm Quitting, in album Hank IV Live at WFMU on Brian's Show on 11/18/2008, by Hank IV, of which the language code is en, the date created is 2008-12-04 20:08:12, the genre is Rock\"]}, {'text': ['playing song titled Rother, Dinger You and Me, by Antiguo Automata Mexicano, of which the date created is 2008-12-04 19:37:15, the language code is en, the genre is Electronic'], 'original_data': {'title': ['FMA: A Dataset For Music Analysis'], 'description': ['Free Music Archive (FMA), an open and easily accessible dataset suitable for evaluating several tasks in MIR, a field concerned with browsing, searching, and organizing large music collections.'], 'license': ['MIT License'], 'filename': ['003345.mp3'], 'genre': ['Electronic'], 'album': ['nan'], 'song_title': ['Rother, Dinger You and Me'], 'artist': ['Antiguo Automata Mexicano'], 'duration': tensor([312]), 'composer': ['nan'], 'date_recorded': ['2008-12-04 19:37:15'], 'language_code': ['en']}, 'seconds_start': tensor([150]), 'seconds_total': tensor([313]), 'prompt': ['playing song titled Rother, Dinger You and Me, by Antiguo Automata Mexicano, of which the date created is 2008-12-04 19:37:15, the language code is en, the genre is Electronic']}, {'text': ['playing song titled Poctoth, in album Mono M::P Free, by Thomas Dimuzio, of which the language code is en, the genre is Electronic, the date created is 2008-11-26 03:21:17'], 'original_data': {'title': ['FMA: A Dataset For Music Analysis'], 'description': ['Free Music Archive (FMA), an open and easily accessible dataset suitable for evaluating several tasks in MIR, a field concerned with browsing, searching, and organizing large music collections.'], 'license': ['MIT License'], 'filename': ['002074.mp3'], 'genre': ['Electronic'], 'album': ['Mono M::P Free'], 'song_title': ['Poctoth'], 'artist': ['Thomas Dimuzio'], 'duration': tensor([126]), 'composer': ['nan'], 'date_recorded': ['2008-11-26 03:21:17'], 'language_code': ['en']}, 'seconds_start': tensor([101]), 'seconds_total': tensor([127]), 'prompt': ['playing song titled Poctoth, in album Mono M::P Free, by Thomas Dimuzio, of which the language code is en, the genre is Electronic, the date created is 2008-11-26 03:21:17']}], [[tensor([0.2057], dtype=torch.float64), tensor([0.2218], dtype=torch.float64)], [tensor([0.2613], dtype=torch.float64), tensor([0.2987], dtype=torch.float64)], [tensor([0.4829], dtype=torch.float64), tensor([0.5004], dtype=torch.float64)], [tensor([0.7989], dtype=torch.float64), tensor([0.8419], dtype=torch.float64)]]]\nSuccess!",
    "crumbs": [
      "datasets"
    ]
  },
  {
    "objectID": "datasets.html#footnotes",
    "href": "datasets.html#footnotes",
    "title": "datasets",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nsic.↩︎",
    "crumbs": [
      "datasets"
    ]
  },
  {
    "objectID": "spectrofu.html",
    "href": "spectrofu.html",
    "title": "spectrofu",
    "section": "",
    "text": "Command-line script that preprocesses a dataset of audio and turns it into spectrograms.\n\nAssumes pre-chunking e.g. via chunkadelic — This is pretty much a simplified duplicate of chunkadelic.\nNote: Duplicates the directory structure(s) referenced by input paths.\n\nsource\n\nsave_stft\n\n save_stft (audio:&lt;built-inmethodtensoroftypeobjectat0x7fa57fa678c0&gt;,\n            new_filename:str)\n\ncoverts audio to stft image and saves it\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\naudio\ntensor\nlong audio file to be chunked\n\n\nnew_filename\nstr\nstem of new filename(s) to be output as spectrogram images\n\n\n\n\nsource\n\n\nmain\n\n main ()\n\n\nsource\n\n\nprocess_one_file\n\n process_one_file (filenames:list, args, file_ind)\n\nthis turns one audio file into a spectrogram. left channel only for now\n\n\n\n\nType\nDetails\n\n\n\n\nfilenames\nlist\nlist of filenames from which we’ll pick one\n\n\nargs\n\noutput of argparse\n\n\nfile_ind\n\nindex from filenames list to read from\n\n\n\n\n! spectrofu -h\n\nusage: spectrofu [-h] [--sr SR] [--workers WORKERS]\n                 output_path input_paths [input_paths ...]\n\npositional arguments:\n  output_path        Path of output for spectrogram-ified data\n  input_paths        Path(s) of a file or a folder of files. (recursive)\n\noptional arguments:\n  -h, --help         show this help message and exit\n  --sr SR            Output sample rate (default: 48000)\n  --workers WORKERS  Maximum number of workers to use (default: all) (default:\n                     14)",
    "crumbs": [
      "spectrofu"
    ]
  },
  {
    "objectID": "viz.html",
    "href": "viz.html",
    "title": "viz",
    "section": "",
    "text": "Originally written for https://github.com/zqevans/audio-diffusion/blob/main/viz/viz.py\nsource",
    "crumbs": [
      "viz"
    ]
  },
  {
    "objectID": "viz.html#d-scatter-plots",
    "href": "viz.html#d-scatter-plots",
    "title": "viz",
    "section": "3D Scatter plots",
    "text": "3D Scatter plots\nTo visualize point clouds in notebook and on WandB:\n\n\nsource\n\npca_point_cloud\n\n pca_point_cloud (tokens, color_scheme='batch', output_type='wandbobj',\n                  mode='markers', size=3, line={'color':\n                  'rgba(10,10,10,0.01)'}, **kwargs)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntokens\n\n\nembeddings / latent vectors. shape = (b, d, n)\n\n\ncolor_scheme\nstr\nbatch\n‘batch’: group by sample, otherwise color sequentially\n\n\noutput_type\nstr\nwandbobj\nplotly | points | wandbobj. NOTE: WandB can do ‘plotly’ directly!\n\n\nmode\nstr\nmarkers\nplotly scatter mode. ‘lines+markers’ or ‘markers’\n\n\nsize\nint\n3\nsize of the dots\n\n\nline\ndict\n{‘color’: ‘rgba(10,10,10,0.01)’}\nif mode=‘lines+markers’, plotly line specifier. cf. https://plotly.github.io/plotly.py-docs/generated/plotly.graph_objects.scatter3d.html#plotly.graph_objects.scatter3d.Line\n\n\nkwargs\n\n\n\n\n\n\n\nsource\n\n\npoint_cloud\n\n point_cloud (tokens, method='pca', color_scheme='batch',\n              output_type='wandbobj', mode='markers', size=3,\n              line={'color': 'rgba(10,10,10,0.01)'}, ds_preproj=1,\n              ds_preplot=1, debug=False, colormap=None, darkmode=False,\n              layout_dict=None, **kwargs)\n\nreturns a 3D point cloud of the tokens\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntokens\n\n\nembeddings / latent vectors. shape = (b, d, n)\n\n\nmethod\nstr\npca\nprojection method for 3d mapping: ‘pca’ | ‘umap’\n\n\ncolor_scheme\nstr\nbatch\n‘batch’: group by sample; integer n: n groups, sequentially, otherwise color sequentially by time step\n\n\noutput_type\nstr\nwandbobj\nplotly | points | wandbobj. NOTE: WandB can do ‘plotly’ directly!\n\n\nmode\nstr\nmarkers\nplotly scatter mode. ‘lines+markers’ or ‘markers’\n\n\nsize\nint\n3\nsize of the dots\n\n\nline\ndict\n{‘color’: ‘rgba(10,10,10,0.01)’}\nif mode=‘lines+markers’, plotly line specifier. cf. https://plotly.github.io/plotly.py-docs/generated/plotly.graph_objects.scatter3d.html#plotly.graph_objects.scatter3d.Line\n\n\nds_preproj\nint\n1\nEXPERIMENTAL: downsampling factor before projecting (1=no downsampling). Could screw up colors\n\n\nds_preplot\nint\n1\nEXPERIMENTAL: downsampling factor before plotting (1=no downsampling). Could screw up colors\n\n\ndebug\nbool\nFalse\nprint more info\n\n\ncolormap\nNoneType\nNone\nvalid color map to use, None=defaults\n\n\ndarkmode\nbool\nFalse\ndark background, white fonts\n\n\nlayout_dict\nNoneType\nNone\nextra plotly layout options such as camera orientation\n\n\nkwargs\n\n\n\n\n\n\nTo display in the notebook (and the online documenation), we need a bit of extra code:\n\nsource\n\n\nshow_pca_point_cloud\n\n show_pca_point_cloud (tokens, color_scheme='batch', mode='markers',\n                       colormap=None, line={'color':\n                       'rgba(10,10,10,0.01)'}, **kwargs)\n\ndisplay a 3d scatter plot of tokens in notebook\n\nsource\n\n\nshow_point_cloud\n\n show_point_cloud (tokens, method='pca', color_scheme='batch',\n                   mode='markers', line={'color': 'rgba(10,10,10,0.01)'},\n                   ds_preproj=1, ds_preplot=1, debug=False, **kwargs)\n\ndisplay a 3d scatter plot of tokens in notebook\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntokens\n\n\nsame arts as point_cloud\n\n\nmethod\nstr\npca\n\n\n\ncolor_scheme\nstr\nbatch\n\n\n\nmode\nstr\nmarkers\n\n\n\nline\ndict\n{‘color’: ‘rgba(10,10,10,0.01)’}\n\n\n\nds_preproj\nint\n1\n\n\n\nds_preplot\nint\n1\n\n\n\ndebug\nbool\nFalse\n\n\n\nkwargs\n\n\n\n\n\n\n\nsource\n\n\nsetup_plotly\n\n setup_plotly (nbdev=True)\n\nPlotly is already ‘setup’ on colab, but on regular Jupyter notebooks we need to do a couple things\n\nsource\n\n\non_colab\n\n on_colab ()\n\nReturns true if code is being executed on Colab, false otherwise\nTest the point cloud viz inside a notebook:\n\nb,d,n = 16,32,152\ntokens = 2*torch.rand((b,d,n))-1\nfor bi in range(b):\n    mu = torch.rand((1,d,1)).tile((1,1,n))\n    tokens[bi,:,:] = mu + 0.7*tokens[bi,:,:]\n\nlayout_dict=dict(  scene=dict(  camera=dict( up=dict(x=0, y=0, z=1), eye=dict(x=0, y=2.0707, z=1), ), ) )\nshow_point_cloud(tokens, darkmode=True,  layout_dict=layout_dict)  # default, no lines connecting dots\n\n\n\n\n                                                \n\n\nTry UMAP:\n\nshow_point_cloud(tokens, method='umap')  # default, no lines connecting dots\n\n                                                \n\n\nOr we can add lines connecting the dots, such as a faint gray line:\n\nshow_point_cloud(tokens, mode='lines+markers')\n\n                                                \n\n\nAnd if we really want 2d instead of 3D, we can flatten to a pancake:\n\nshow_point_cloud(tokens, mode='lines+markers', method='umap', proj_dims=2)",
    "crumbs": [
      "viz"
    ]
  },
  {
    "objectID": "viz.html#print-audio-info",
    "href": "viz.html#print-audio-info",
    "title": "viz",
    "section": "Print audio info",
    "text": "Print audio info\n\nsource\n\nprint_stats\n\n print_stats (waveform, sample_rate=None, src=None, print=&lt;built-in\n              function print&gt;)\n\nprint stats about waveform. Taken verbatim from pytorch docs.\nTesting that:\n\naudio_filename = 'examples/example.wav'\nwaveform = load_audio(audio_filename)\nprint_stats(waveform)\n\nResampling examples/example.wav from 44100 Hz to 48000 Hz\nShape: (1, 55728)\nDtype: torch.float32\n - Max:      0.647\n - Min:     -0.647\n - Mean:     0.000\n - Std Dev:  0.075\n\ntensor([[-3.0239e-04, -3.8517e-04, -6.0043e-04,  ...,  2.4789e-05,\n         -1.3458e-04, -8.0428e-06]])",
    "crumbs": [
      "viz"
    ]
  },
  {
    "objectID": "viz.html#spectrograms",
    "href": "viz.html#spectrograms",
    "title": "viz",
    "section": "Spectrograms",
    "text": "Spectrograms\n\nsource\n\nmel_spectrogram\n\n mel_spectrogram (waveform, power=2.0, sample_rate=48000, db=False,\n                  n_fft=1024, n_mels=128, debug=False)\n\ncalculates data array for mel spectrogram (in however many channels)\n\nsource\n\n\nspectrogram_image\n\n spectrogram_image (spec, title=None, ylabel='freq_bin', aspect='auto',\n                    xmax=None, db_range=[35, 120], justimage=False,\n                    figsize=(5, 4))\n\nModified from PyTorch tutorial https://pytorch.org/tutorials/beginner/audio_feature_extractions_tutorial.html\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nspec\n\n\n\n\n\ntitle\nNoneType\nNone\n\n\n\nylabel\nstr\nfreq_bin\n\n\n\naspect\nstr\nauto\n\n\n\nxmax\nNoneType\nNone\n\n\n\ndb_range\nlist\n[35, 120]\n\n\n\njustimage\nbool\nFalse\n\n\n\nfigsize\ntuple\n(5, 4)\nsize of plot (if justimage==False)\n\n\n\n\nsource\n\n\naudio_spectrogram_image\n\n audio_spectrogram_image (waveform, power=2.0, sample_rate=48000,\n                          print=&lt;built-in function print&gt;, db=False,\n                          db_range=[35, 120], justimage=False, log=False,\n                          figsize=(5, 4))\n\nWrapper for calling above two routines at once, does Mel scale; Modified from PyTorch tutorial https://pytorch.org/tutorials/beginner/audio_feature_extractions_tutorial.html\nLet’s test the above routine:\n\nspec_graph = audio_spectrogram_image(waveform, justimage=False, db=False, db_range=[-60,20])\ndisplay(spec_graph)\n\n/Users/shawley/envs/aa/lib/python3.10/site-packages/torchaudio/transforms/_transforms.py:611: UserWarning:\n\nArgument 'onesided' has been deprecated and has no influence on the behavior of this module.\n\n/Users/shawley/envs/aa/lib/python3.10/site-packages/torchaudio/functional/functional.py:576: UserWarning:\n\nAt least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (513) may be set too low.\n\n\n\n\n\n\n\n\n\n\n\n\n‘Playable Spectrograms’\nSource(s): Original code by Scott Condron (@scottire) of Weights and Biases, edited by @drscotthawley\ncf. @scottire’s original code here: https://gist.github.com/scottire/a8e5b74efca37945c0f1b0670761d568\nand Morgan McGuire’s edit here; https://github.com/morganmcg1/wandb_spectrogram\n\nsource\n\n\nplayable_spectrogram\n\n playable_spectrogram (waveform, sample_rate=48000, specs:str='all',\n                       layout:str='row', height=170, width=400,\n                       cmap='viridis', output_type='wandb', debug=True)\n\n*Takes a tensor input and returns a [wandb.]HTML object with spectrograms of the audio specs : “all_specs”, spectrograms only “all”, all plots “melspec”, melspectrogram only “spec”, spectrogram only “wave_mel”, waveform and melspectrogram only “waveform”, waveform only, equivalent to wandb.Audio object\nLimitations: spectrograms show channel 0 only (i.e., mono)*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nwaveform\n\n\naudio, PyTorch tensor\n\n\nsample_rate\nint\n48000\nsample rate in Hz\n\n\nspecs\nstr\nall\nsee docstring below\n\n\nlayout\nstr\nrow\n‘row’ or ‘grid’\n\n\nheight\nint\n170\nheight of spectrogram image\n\n\nwidth\nint\n400\nwidth of spectrogram image\n\n\ncmap\nstr\nviridis\ncolormap string for Holoviews, see https://holoviews.org/user_guide/Colormaps.html\n\n\noutput_type\nstr\nwandb\n‘wandb’, ‘html_file’, ‘live’: use live for notebooks\n\n\ndebug\nbool\nTrue\nflag for internal print statements\n\n\n\n\nsource\n\n\ngenerate_melspec\n\n generate_melspec (audio_data, sample_rate=48000, power=2.0, n_fft=1024,\n                   win_length=None, hop_length=None, n_mels=128)\n\nhelper routine for playable_spectrogram\nSample usage with WandB:\nwandb.init(project='audio_test')\nwandb.log({\"playable_spectrograms\": playable_spectrogram(waveform)})\nwandb.finish()\nSee example result at https://wandb.ai/drscotthawley/playable_spectrogram_test/\nTest the playable spectrogram. Let’s show off the multichannel waveform display:\n\nmc_wave = load_audio('examples/stereo_pewpew.mp3')\nplayable_spectrogram(mc_wave, specs='wave_mel', output_type='live')\n\nResampling examples/stereo_pewpew.mp3 from 44100.0 Hz to 48000 Hz\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nsource\n\n\ntokens_spectrogram_image\n\n tokens_spectrogram_image (tokens, aspect='auto', title='Embeddings',\n                           ylabel='index', cmap='coolwarm',\n                           symmetric=True, figsize=(8, 4), dpi=100,\n                           mark_batches=False, debug=False)\n\nfor visualizing embeddings in a spectrogram-like way\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntokens\n\n\nthe embeddings themselves (in some diffusion codes these are called ‘tokens’)\n\n\naspect\nstr\nauto\naspect ratio of plot\n\n\ntitle\nstr\nEmbeddings\ntitle to put on top\n\n\nylabel\nstr\nindex\nlabel for y axis of plot\n\n\ncmap\nstr\ncoolwarm\ncolormap to use. (default used to be ‘viridis’)\n\n\nsymmetric\nbool\nTrue\nmake color scale symmetric about zero, i.e. +/- same extremes\n\n\nfigsize\ntuple\n(8, 4)\nmatplotlib size of the figure\n\n\ndpi\nint\n100\ndpi of figure\n\n\nmark_batches\nbool\nFalse\nseparate batches with dividing lines\n\n\ndebug\nbool\nFalse\nprint debugging info\n\n\n\n\ntokens = 2*torch.rand((16,32,152))-1 + 0.257  # make embeddings a bit 'off center' for demo purposes\n\ntokens_spectrogram_image(tokens, mark_batches=True)  # new default symmetrizes max/min of colormap and uses 'coolwarm' colors.\n\n\n\n\n\n\n\n\nThe default behavior from earlier versions of aeiou can be recovered via the following kwarg settings:\n\ntokens_spectrogram_image(tokens, cmap='viridis', symmetric=False) # this was the old  default behavior (v0.0.15 & earlier)\n\n\n\n\n\n\n\n\n\nsource\n\n\nplot_jukebox_embeddings\n\n plot_jukebox_embeddings (zs, aspect='auto')\n\nmakes a plot of jukebox embeddings",
    "crumbs": [
      "viz"
    ]
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "source",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "core.html#load_audio",
    "href": "core.html#load_audio",
    "title": "core",
    "section": "load_audio",
    "text": "load_audio\nWe’ll start with a basic utilty to read an audio file. If it’s not at the sample rate we want, we’ll automatically resample it. Note that if you want MP3 support, you’ll need to install ffmpeg system-wide first.\n\nsource\n\nload_audio\n\n load_audio (filename:str, sr=48000, verbose=True, norm='')\n\nloads an audio file as a torch tensor\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfilename\nstr\n\nname of file to load\n\n\nsr\nint\n48000\nsample rate in Hz\n\n\nverbose\nbool\nTrue\nwhether or not to print notices of resampling\n\n\nnorm\nstr\n\npassedto normalize_audio(), see above\n\n\nReturns\ntensor\n\n\n\n\n\nUsing the file in examples/, let’s see how this works:\n\naudio = load_audio('examples/example.wav')\n\nResampling examples/example.wav from 44100 Hz to 48000 Hz\n\n\n\naudio = load_audio('examples/example.wav',verbose=False)\n\nLet’s check to see if we can read MP3s:\n\nfor norm in ['','global','channel']:\n    audio = load_audio('examples/stereo_pewpew.mp3',verbose=False, norm=norm)\n    print(f\"norm = {norm}: shape = \",audio.shape, \"Per-channel abs-maxes are : \", np.abs(audio.numpy()).max(axis=-1))\n\nnorm = : shape =  torch.Size([2, 236983]) Per-channel abs-maxes are :  [0.8505264  0.50114477]\nnorm = global: shape =  torch.Size([2, 236983]) Per-channel abs-maxes are :  [0.98999995 0.583325  ]\nnorm = channel: shape =  torch.Size([2, 236983]) Per-channel abs-maxes are :  [0.98999995 0.99      ]\n\n\nNote that pedalboard could be used to read any of the following types of files…\n\nprint(pdlbd_exts)\n\n['.aif', '.aiff', '.bwf', '.flac', '.mp3', '.ogg', '.wav']\n\n\n…but we’re only using it for MP3s right now, and torchaudio for everything else.",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "core.html#get_dbmax",
    "href": "core.html#get_dbmax",
    "title": "core",
    "section": "get_dbmax",
    "text": "get_dbmax\nFinds loudest sample value in the entire clip and returns the value as decibels\n\nsource\n\nget_dbmax\n\n get_dbmax (audio)\n\nfinds the loudest value in the entire clip and puts that into dB (full scale)\n\n\n\n\nDetails\n\n\n\n\naudio\ntorch tensor of (multichannel) audio\n\n\n\n\nprint(\"dbmax of last-loaded audio is\",get_dbmax(audio))\n\ndbmax of last-loaded audio is -0.08729602210223675",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "core.html#is_silence",
    "href": "core.html#is_silence",
    "title": "core",
    "section": "is_silence",
    "text": "is_silence\nSometimes we’ll want to know if a file is “silent”, i.e. if its contents are quieter than some threshold. Here’s one simple way to implement that:\n\nsource\n\naudio_float_to_int\n\n audio_float_to_int (waveform)\n\nconverts torch float to numpy int16 (for playback in notebooks)\n\nprint(audio.dtype)\nprint(audio_float_to_int(audio).dtype)\n\ntorch.float32\nint16\n\n\n\nsource\n\n\nis_silence\n\n is_silence (audio, thresh=-60)\n\nchecks if entire clip is ‘silence’ below some dB threshold\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\naudio\n\n\ntorch tensor of (multichannel) audio\n\n\nthresh\nint\n-60\nthreshold in dB below which we declare to be silence\n\n\n\nLet’s test that with some tests. If all goes well, the following assert statements will all pass uneventfully.\n\nx = torch.ones((2,10))\nassert not is_silence(1e-3*x) # not silent\nassert is_silence(1e-5*x) # silent\nassert is_silence(1e-3*x, thresh=-50) # higher thresh",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "core.html#batch_it_crazy",
    "href": "core.html#batch_it_crazy",
    "title": "core",
    "section": "batch_it_crazy",
    "text": "batch_it_crazy\nThis is a pretty basic utility for breaking up a long sequence into batches, e.g. for model inference\n\nsource\n\nbatch_it_crazy\n\n batch_it_crazy (x, win_len)\n\n(pun intended) Chop up long sequence into a batch of win_len windows\n\n\n\n\n\n\n\n\nDetails\n\n\n\n\nx\na time series as a PyTorch tensor, e.g. stereo or mono audio\n\n\nwin_len\nlength of each “window”, i.e. length of each element in new batch\n\n\n\nTesting batch_it_crazy() for stereo input:\n\nx = torch.ones([2,1000])  # stereo\nbatch_it_crazy(x, 10).shape\n\ntorch.Size([101, 2, 10])\n\n\n…and for mono:\n\nx = torch.ones([1000])   # mono\nbatch_it_crazy(x, 10).shape\n\ntorch.Size([101, 1, 10])\n\n\n…and yeah, currently that “1,” stays because other parts of the code(s) will be assuming “multichannel” audio.",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "core.html#makedir",
    "href": "core.html#makedir",
    "title": "core",
    "section": "makedir",
    "text": "makedir\nThe next routine creates a directory if it doesn’t already exist. We’ll even let it take a “nested” directory such as a/b/c/d and the routine will create any directories in that string.\n\nsource\n\nmakedir\n\n makedir (path:str)\n\ncreates directories where they don’t exist\n\n\n\n\nType\nDetails\n\n\n\n\npath\nstr\ndirectory or nested set of directories",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "core.html#get_audio_filenames",
    "href": "core.html#get_audio_filenames",
    "title": "core",
    "section": "get_audio_filenames",
    "text": "get_audio_filenames\nOften we’ll want to grab a long list of audio filenames by looking through a directory and all its subdirectories. We could use something like glob, glob turns out to be extremely slow when large numbers of files (say, more than 100,000) are involved. Instead we will use the much faster os.scandir(), which was packaged nicely into the following routine in an answer to a StackOverflow question from which this code is modified:\n\nsource\n\nfast_scandir\n\n fast_scandir (dir:str, ext:list)\n\nvery fast glob alternative. from https://stackoverflow.com/a/59803793/4259243\n\n\n\n\nType\nDetails\n\n\n\n\ndir\nstr\ntop-level directory at which to begin scanning\n\n\next\nlist\nlist of allowed file extensions\n\n\n\nQuick test:\n\n_, files = fast_scandir('examples/', ['wav','flac','ogg','aiff','aif','mp3'])\nfiles\n\n['examples/stereo_pewpew.mp3', 'examples/example.wav']\n\n\nOften, rather than being given a single parent directory, we may be given a list of directories in which to look for files. The following just called fast_scandir() for each of those:\n\nsource\n\n\nget_audio_filenames\n\n get_audio_filenames (paths:list)\n\nrecursively get a list of audio filenames\n\n\n\n\nType\nDetails\n\n\n\n\npaths\nlist\ndirectories in which to search\n\n\n\nHere’s a fun trick to show off how fast this is: Run in the user’s directory tree:\n\npath = str(os.path.expanduser(\"~\"))+'/Downloads'\nif os.path.exists(path):\n    files = get_audio_filenames(path)\n    print(f\"Found {len(files)} audio files.\")\nelse:\n    print(\"Ok it was just a thought.\")\n\nOk it was just a thought.\n\n\n\nsource\n\n\nuntuple\n\n untuple (x, verbose=False)\n\nRecursive. For when you’re sick of tuples and lists: keeps peeling off elements until we get a non-tuple or non-list, i.e., returns the ‘first’ data element we can ‘actually use’\n\na = [[((5,6),7)]]\nprint(a)\nprint(untuple(a, verbose=True))\n\n[[((5, 6), 7)]]\nyea: x =  [[((5, 6), 7)]]\nyea: x =  [((5, 6), 7)]\nyea: x =  ((5, 6), 7)\nyea: x =  (5, 6)\nno: x =  5\n5",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "core.html#run-names-and-checkpointing",
    "href": "core.html#run-names-and-checkpointing",
    "title": "core",
    "section": "Run Names and Checkpointing",
    "text": "Run Names and Checkpointing\nin concert with Pytorch Lightning\n\nsource\n\nget_latest_ckpt\n\n get_latest_ckpt (dir_tree, run_name_prefix='', sim_ckpts=[''],\n                  verbose=True)\n\nThis will grab the most recent checkpoint filename in dir tree given by name\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndir_tree\n\n\nname of the run without unique identifer\n\n\nrun_name_prefix\nstr\n\nunique identifier for particular run\n\n\nsim_ckpts\nlist\n[’’]\nstring or list of strings. other paths to check under if nothing’s in dir_tree\n\n\nverbose\nbool\nTrue\nwhether to print message(s)\n\n\n\n\n# testing\nget_latest_ckpt('/fsx/shawley/runs/clapg88s', \n                run_name_prefix='songlike', \n                sim_ckpts='/fsx/shawley/runs/longer-songs2-stacked-clap-audio')\n\n   Nothing relevant found in /fsx/shawley/runs/clapg88s. Checking also in /fsx/shawley/runs/longer-songs2-stacked-clap-audio.\n   pattern =  /fsx/shawley/runs/longer-songs2-stacked-clap-audio\n   Also checking in  ['/fsx/shawley/runs/longer-songs2-stacked-clap-audio']\n     directory =  /fsx/shawley/runs/longer-songs2-stacked-clap-audio\n\n\nPath('/fsx/shawley/runs/longer-songs2-stacked-clap-audio/songlike_0eac0fa8/checkpoints/epoch=10-step=100000.ckpt')\n\n\n\n#testing\nname = '/fsx/shawley/runs/longer-songs2-stacked-clap-audio'\nckpt_path = get_latest_ckpt(name, run_name_prefix='')\nckpt_path\n\nLooking for latest checkpoint in /fsx/shawley/runs/longer-songs2-stacked-clap-audio/*/checkpoints/*.ckpt\n\n\n'/fsx/shawley/runs/longer-songs2-stacked-clap-audio/songlike_0eac0fa8/checkpoints/epoch=10-step=100000.ckpt'\n\n\n\nsource\n\n\nget_run_info\n\n get_run_info (run_name, verbose=True)\n\nparses run_name into (ideally) prefix & id using underscore as separator, and/or fills in missing info if needed NOTE: do not trust generated strings to be same on other processes\n\nsource\n\n\nrnd_string\n\n rnd_string (n=8)\n\nrandom letters and numbers of given length. case sensitive\n\nget_run_info('songlike_345876jh')\n\n{'prefix': 'songlike', 'id': '345876jh', 'run_name': 'songlike_345876jh'}\n\n\nSample usage of the previous few functions:\n    # Reading from OLD checkpoint to start\n    run_info = get_run_info(args.run_name)\n    grab_latest_checkpoint = True\n    if grab_latest_checkpoint:\n        print(\"Looking for old checkpoint to load for startup\")\n        ckpt_path = get_latest_ckpt(args.name, run_name_prefix=run_info['prefix']) \n        if os.path.exists(ckpt_path):\n            print(f\"Found latest checkpoint at {ckpt_path}\")\n            args.ckpt_path = ckpt_path\n    if args.ckpt_path:\n        print(f\"Loading model from {args.ckpt_path}\")\n        model = OurModel.load_from_checkpoint(args.ckpt_path, latent_ae=latent_diffae, clap_module=clap_module, strict=False)\n    else:\n        model = OurModel(latent_ae=latent_diffae, clap_module=clap_module)\n\n    \n    # Where to save NEW checkpoints\n    ckpt_dir = f\"{args.name}/{run_info['run_name']}/checkpoints\" \n    print(f\"New checkpoints will be saved in {ckpt_dir}\")\n    ckpt_callback = pl.callbacks.ModelCheckpoint(dirpath=ckpt_dir, every_n_train_steps=args.checkpoint_every, save_top_k=-1, save_last=True)\n    \n    wandb_logger = pl.loggers.WandbLogger(project=args.name, id=run_info['id']) \n    wandb_logger.watch(latent_diffusion_model)\n    push_wandb_config(wandb_logger, args)",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "aeiou",
    "section": "",
    "text": "Pronounced “ayoo”",
    "crumbs": [
      "aeiou"
    ]
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "aeiou",
    "section": "Install",
    "text": "Install\nIt is recommended you install the latest version from GitHub via\npip install git+https://github.com/drscotthawley/aeiou.git\nHowever binaries will be occasionally updated on PyPI, installed via\npip install aeiou",
    "crumbs": [
      "aeiou"
    ]
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "aeiou",
    "section": "How to use",
    "text": "How to use\nThis is a series of utility routines developed in support of multiple projects within the Harmonai organization. See individual documentation pages for more specific instructions on how these can be used. Note that this is research code, so it’s a) in flux and b) in need of improvements to documenation.",
    "crumbs": [
      "aeiou"
    ]
  },
  {
    "objectID": "index.html#documentation",
    "href": "index.html#documentation",
    "title": "aeiou",
    "section": "Documentation",
    "text": "Documentation\nDocumentation for this library is hosted on the aeiou GitHub Pages site.",
    "crumbs": [
      "aeiou"
    ]
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "aeiou",
    "section": "Contributing",
    "text": "Contributing\nContributions are welcome – especially for improvements to documentation! To contribute:\n\nFork this repo and then clone your fork to your local machine.\nCreate a new (local) branch: git -b mybranch (or whatever you want to call it).\nThis library is written entirely in nbdev version 2, using Jupyter notebooks.\nInstall nbdev and then you can edit the Jupyter notebooks.\nAfter editing notebooks, run nbdev_prepare\nIf that succeeds, you can do git add *.ipynb aeiou/*.py; git commit and then git push to get your changes to back to your fork on GitHub.\nThen send a Pull Request from your fork to the main aeiou repository.",
    "crumbs": [
      "aeiou"
    ]
  },
  {
    "objectID": "index.html#attribution",
    "href": "index.html#attribution",
    "title": "aeiou",
    "section": "Attribution",
    "text": "Attribution\nPlease include attribution of this code if you reproduce sections of it in your own code:\naeiou: audio engineering i/o utilities: Copyright (c) Scott H. Hawley, 2022-2023. https://github.com/drscotthawley/aeiou\nIn research papers, please cite this software if you find it useful:\n@misc{aeiou,\n  author = {Scott H. Hawley},\n  title = {aeiou: audio engineering i/o utilities},\n  year  = {2022},\n  url   = {https://github.com/drscotthawley/aeiou},\n}\nCopyright (c) Scott H. Hawley, 2022-2023.",
    "crumbs": [
      "aeiou"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "aeiou",
    "section": "License",
    "text": "License\nLicense is APACHE 2.0.",
    "crumbs": [
      "aeiou"
    ]
  },
  {
    "objectID": "hpc.html",
    "href": "hpc.html",
    "title": "hpc",
    "section": "",
    "text": "This part isn’t strictly for audio i/o, but is nevertheless a normal part of Harmonai’s operations. The point of this package is to reduce code-copying between Harmonai projects.\nHeads up: Huggingface accelerate support will likely be deprecated soon. We found accelerate necessary because of problems running PyTorch Lightning on multiple nodes, but those problems have now been resolved. Thus we will likely be using Lighting, so you will see that dependency being added and perhaps accelerate being removed.\nsource",
    "crumbs": [
      "hpc"
    ]
  },
  {
    "objectID": "hpc.html#pytorchaccelerate-model-routines",
    "href": "hpc.html#pytorchaccelerate-model-routines",
    "title": "hpc",
    "section": "PyTorch+Accelerate Model routines",
    "text": "PyTorch+Accelerate Model routines\nFor when the model is wrapped in a accelerate accelerator\n\nsource\n\nsave\n\n save (accelerator, args, model, opt=None, epoch=None, step=None)\n\nfor checkpointing & model saves\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\naccelerator\n\n\nHuggingface accelerator object\n\n\nargs\n\n\nprefigure args dict, (we only use args.name)\n\n\nmodel\n\n\nthe model, pre-unwrapped\n\n\nopt\nNoneType\nNone\noptimizer state\n\n\nepoch\nNoneType\nNone\ntraining epoch number\n\n\nstep\nNoneType\nNone\ntraining setp number\n\n\n\n\nsource\n\n\nload\n\n load (accelerator, model, filename:str, opt=None)\n\nload a saved model checkpoint\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\naccelerator\n\n\nHuggingface accelerator object\n\n\nmodel\n\n\nan uninitialized model (pre-unwrapped) whose weights will be overwritten\n\n\nfilename\nstr\n\nname of the checkpoint file\n\n\nopt\nNoneType\nNone\noptimizer state UNUSED FOR NOW",
    "crumbs": [
      "hpc"
    ]
  },
  {
    "objectID": "hpc.html#utils-for-accelerate-or-lightning",
    "href": "hpc.html#utils-for-accelerate-or-lightning",
    "title": "hpc",
    "section": "Utils for Accelerate or Lightning",
    "text": "Utils for Accelerate or Lightning\nBe sure to use “unwrap” any accelerate model when calling these\n\nsource\n\nn_params\n\n n_params (module)\n\nReturns the number of trainable parameters in a module. Be sure to use accelerator.unwrap_model when calling this.\n\n\n\n\n\n\n\n\nDetails\n\n\n\n\nmodule\nraw PyTorch model/module, e.g. returned by accelerator.unwrap_model()\n\n\n\n\nsource\n\n\nfreeze\n\n freeze (model)\n\nfreezes model weights; turns off gradient info If using accelerate, call thisaccelerator.unwrap_model when calling this.\n\n\n\n\n\n\n\n\nDetails\n\n\n\n\nmodel\nraw PyTorch model, e.g. returned by accelerator.unwrap_model()",
    "crumbs": [
      "hpc"
    ]
  },
  {
    "objectID": "chunkadelic.html",
    "href": "chunkadelic.html",
    "title": "chunkadelic",
    "section": "",
    "text": "Console script and callable function for preprocessing dataset of disparate-sized audio files into uniform chunks\n\nNote: Duplicates the directory structure(s) referenced by input paths.\n$ chunkadelic -h \nusage: chunkadelic [-h] [--chunk_size CHUNK_SIZE] [--sr SR] [--norm [{False,global,channel}]] [--spacing SPACING] [--strip] [--thresh THRESH]\n                   [--bits BITS] [--workers WORKERS] [--nomix] [--nopad] [--verbose] [--debug]\n                   output_path input_paths [input_paths ...]\n\npositional arguments:\n  output_path           Path of output for chunkified data\n  input_paths           Path(s) of a file or a folder of files. (recursive)\n\noptions:\n  -h, --help            show this help message and exit\n  --chunk_size CHUNK_SIZE\n                        Length of chunks (default: 131072)\n  --sr SR               Output sample rate (default: 48000)\n  --norm [{False,global,channel}]\n                        Normalize audio, based on the max of the absolute value [global/channel/False] (default: False)\n  --spacing SPACING     Spacing factor, advance this fraction of a chunk per copy (default: 0.5)\n  --strip               Strips silence: chunks with max dB below &lt;thresh&gt; are not outputted (default: False)\n  --thresh THRESH       threshold in dB for determining what constitutes silence (default: -70)\n  --bits BITS           Bit depth: \"None\" uses torchaudio default | \"match\"=match input audio files | or specify an int (default: None)\n  --workers WORKERS     Maximum number of workers to use (default: all) (default: 20)\n  --nomix               (BDCT Dataset specific) exclude output of \"*/Audio Files/*Mix*\" (default: False)\n  --nopad               Disable zero padding for audio shorter than chunk_size (default: False)\n  --verbose             Extra output logging (default: False)\n  --debug               Extra EXTRA output logging (default: False)\n\nsource\n\nblow_chunks\n\n blow_chunks (audio:&lt;built-inmethodtensoroftypeobjectat0x7f40f98678c0&gt;,\n              new_filename:str, chunk_size:int, sr=48000, norm='False',\n              spacing=0.5, strip=False, thresh=-70, bits_per_sample=None,\n              nopad=False, debug=False)\n\nchunks up the audio and saves them with –{i} on the end of each chunk filename\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\naudio\ntensor\n\nlong audio file to be chunked\n\n\nnew_filename\nstr\n\nstem of new filename(s) to be output as chunks\n\n\nchunk_size\nint\n\nhow big each audio chunk is, in samples\n\n\nsr\nint\n48000\naudio sample rate in Hz\n\n\nnorm\nstr\nFalse\nnormalize input audio, based on the max of the absolute value [‘global’,‘channel’, or anything else for None, e.g. False]\n\n\nspacing\nfloat\n0.5\nfraction of each chunk to advance between hops\n\n\nstrip\nbool\nFalse\nstrip silence: chunks with max power in dB below this value will not be saved to files\n\n\nthresh\nint\n-70\nthreshold in dB for determining what counts as silence\n\n\nbits_per_sample\nNoneType\nNone\nkwarg for torchaudio.save, None means use defaults\n\n\nnopad\nbool\nFalse\ndisable zero-padding, allowing samples to be shorter than chunk_size (including “leftovers” on the “ends”)\n\n\ndebug\nbool\nFalse\nprint debugging information\n\n\n\n\nsource\n\n\nset_bit_rate\n\n set_bit_rate (bits, filename, debug=False)\n\n\nsource\n\n\nchunk_one_file\n\n chunk_one_file (filenames:list, args, file_ind)\n\nthis chunks up one file by setting things up and then calling blow_chunks\n\n\n\n\nType\nDetails\n\n\n\n\nfilenames\nlist\nlist of filenames from which we’ll pick one\n\n\nargs\n\noutput of argparse\n\n\nfile_ind\n\nindex from filenames list to read from\n\n\n\nTesting sequential execution of for one file at a time:\n\nclass AttrDict(dict): # cf. https://stackoverflow.com/a/14620633/4259243\n    \"setup an object to hold args\"\n    def __init__(self, *args, **kwargs):\n        super(AttrDict, self).__init__(*args, **kwargs)\n        self.__dict__ = self\n        \nargs = AttrDict()  # setup something akin to what argparse gives\nargs.update( {'output_path':'test_chunks', 'input_paths':['examples/'], 'sr':48000, 'chunk_size':131072, 'spacing':0.5,\n    'norm':'global', 'strip':False, 'thresh':-70, 'nomix':False, 'verbose':True, 'nopad':True,\n    'workers':min(32, os.cpu_count() + 4), 'debug':True, 'bits':'match' })\n\nfilenames = get_audio_filenames(args.input_paths)\nprint(\"filenames =\",filenames)\nfor i in range(len(filenames)):\n    print(f\"file {i+1}/{len(filenames)}: {filenames[i]}:\")\n    chunk_one_file(filenames, args, i)\n\nfilenames = ['examples/stereo_pewpew.mp3', 'examples/example.wav']\nfile 1/2: examples/stereo_pewpew.mp3:\n --- process_one_file: filenames[0] = examples/stereo_pewpew.mp3\n\n   About to load filenames[0] = examples/stereo_pewpew.mp3\n\nResampling examples/stereo_pewpew.mp3 from 44100.0 Hz to 48000 Hz\n   We loaded the audio, audio.shape = torch.Size([2, 236983]).  Setting bit rate.\n     Error with bits=match: Can't get audio medatadata. Choosing default=None\n     set_bit_rate: bits_per_sample = None\n   Bit rate set.  Calling blow_chunks...\n       blow_chunks: audio.shape = torch.Size([2, 236983])\n     Saving output chunk test_chunks/stereo_pewpew--0.mp3, bits_per_sample=None, chunk.shape=torch.Size([2, 131072])\n     Saving output chunk test_chunks/stereo_pewpew--1.mp3, bits_per_sample=None, chunk.shape=torch.Size([2, 131072])\n     Saving output chunk test_chunks/stereo_pewpew--2.mp3, bits_per_sample=None, chunk.shape=torch.Size([2, 105911])\n     Saving output chunk test_chunks/stereo_pewpew--3.mp3, bits_per_sample=None, chunk.shape=torch.Size([2, 40375])\n --- File 0: examples/stereo_pewpew.mp3 completed.\n\nfile 2/2: examples/example.wav:\n --- process_one_file: filenames[1] = examples/example.wav\n\n   About to load filenames[1] = examples/example.wav\n\nResampling examples/example.wav from 44100 Hz to 48000 Hz\n   We loaded the audio, audio.shape = torch.Size([1, 55728]).  Setting bit rate.\n     set_bit_rate: bits_per_sample = 16\n   Bit rate set.  Calling blow_chunks...\n       blow_chunks: audio.shape = torch.Size([1, 55728])\n     Saving output chunk test_chunks/example--0.wav, bits_per_sample=16, chunk.shape=torch.Size([1, 55728])\n --- File 1: examples/example.wav completed.\n\n\n\nThe main executable chunkadelic does the same as the previous sequential execution, albeit in parallel.\n\nNote: Restrictions in Python’s ProcessPoolExecutor prevent directly invoking parallel execution of chunk_one_file while in interactive mode or inside a Jupyter notebook: You must use the CLI (or subprocess it).\n\n\nsource\n\n\nmain\n\n main ()\n\n\n\n\n\n\n\nTesting of CLI run: (don’t run this on GitHub CI or it will hang)\n\n\n\n\nnorm = False\n\n\n\nnorm = global\n\nnorm = channel\n``` ::: :::",
    "crumbs": [
      "chunkadelic"
    ]
  }
]